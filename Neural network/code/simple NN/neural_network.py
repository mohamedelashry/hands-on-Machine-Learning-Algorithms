# -*- coding: utf-8 -*-
"""Neural_Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GDN6sIvTJ44puNxZfuNjPn7baMm3qhR-
"""

import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import sys

class NeuralNetwork():

    def __init__(self, n_inputs, n_hiddens_list, n_outputs):

        self.n_inputs = n_inputs
        self.n_hiddens_list = n_hiddens_list
        self.n_outputs = n_outputs
        self.n_layers = len(n_hiddens_list) + 1

        self.all_weights, self.Ws = self.make_weights()
        self.initialize_weights()
        self.all_gradients, self.Gs = self.make_weights()

        self.stand_params = None
        self.error_trace = []

    def __repr__(self):
        return f'NeuralNetwork({self.n_inputs}, {self.n_hiddens_list}, {self.n_outputs})'

    def make_weights(self):
        n_in = self.n_inputs
        units = self.n_hiddens_list + [self.n_outputs]
        shapes = []
        for n_units in units:
            shapes.append((n_in + 1, n_units))
            n_in = n_units

        # hold all weights
        n_weights = sum([ni * nu for (ni, nu) in shapes])
        all_weights = np.zeros(n_weights)
        
        # shapes to be our weight matrices.
        Ws = []
        first = 0
        for (ni, nu) in shapes:
            n_w = ni * nu
            W = all_weights[first:first + n_w].reshape(ni, nu)
            Ws.append(W)
            first += n_w
            
        return all_weights, Ws

   
    def add_ones(self, X):
        return np.insert(X, 0, 1, axis=1)
    
    def initialize_weights(self):
        for W in self.Ws:
            ni, nu = W.shape
            W[:] = np.random.uniform(-1, 1, size=(ni, nu)) / np.sqrt(ni)
        self.error_trace = []
    
    

    def calc_standardize_parameters(self, X, T):
        Xmeans = X.mean(axis=0)
        Xstds = X.std(axis=0)
        Xstds[Xstds == 0] = np.mean(Xstds[Xstds > 0])
        Tmeans = T.mean(axis=0)
        Tstds = T.std(axis=0)
        return {'Xmeans': Xmeans, 'Xstds': Xstds, 'Tmeans': Tmeans, 'Tstds': Tstds}

    def standardize_X(self, X):
        return (X - self.stand_params['Xmeans']) / self.stand_params['Xstds']

    def unstandardize_X(self, Xst):
        return Xst * self.stand_params['Xstds'] + self.stand_params['Xmeans']

    def standardize_T(self, T):
        return (T - self.stand_params['Tmeans']) / self.stand_params['Tstds']

    def unstandardize_T(self, Tst):
        return Tst * self.stand_params['Tstds'] + self.stand_params['Tmeans']

class NeuralNetworkClassifier(NeuralNetwork):       
    def forward(self,Xst):
        Ys = [Xst]
        for layer_i, W in enumerate(self.Ws):
            X = Ys[-1]
            Y = self.add_ones(X) @ W
            # use tanh for hidden units and softmax for output units
            if(layer_i != (len(self.Ws)-1) ) :
              Y = np.tanh(Y)

            else:
              Y = self.softmax(Y)
            Ys.append(Y)
        return Ys[1:] 

    def backward(self, Xst, Tst):
        n_samples = Xst.shape[0]
        Ys = self.forward(Xst)
        Ys = [Xst] + Ys  
        delta = -(Tst - Ys[-1]) / (n_samples * self.n_outputs)
        for layeri in range(self.n_layers)[::-1]:
            X = Ys[layeri]
            self.Gs[layeri][:] = self.add_ones(X).T @ delta
            if layeri > 0:
                delta = delta @ self.Ws[layeri][1:, :].T
                delta *= 1 - Ys[layeri] ** 2
        return self.all_gradients
    
    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, save_wtrace=False,
            verbose=True, error_convert_f=None):
        error_trace = []
        weights_trace = []
        if save_wtrace:
            weights_trace = [self.all_weights.copy()]
        epochs_print = 5

        for epoch in range(n_epochs):

            error = error_f(*fargs)
            grad = gradient_f(*fargs)

            # Update all weights
            self.all_weights -= learning_rate * grad

            if error_convert_f:
                error = error_convert_f(error)
            error_trace.append(error)

            if save_wtrace:
                weights_trace.append(self.all_weights.copy())

            if verbose and ((epoch + 1) % epochs_print == 0):
                error_scalar = np.asscalar(error) if isinstance(error, np.ndarray) else error
                print(f'sgd: Epoch {epoch+1:d} Error={error_scalar:.5f}')

        return (error_trace, np.array(weights_trace)) if save_wtrace else error_trace


    def train(self, X, T, n_epochs, learning_rate=0.01, verbose=True):

        self.stand_params = self.calc_standardize_parameters(X, T)
        Xst = self.standardize_X(X)
        
        self.classes, counts = np.unique(T, return_counts=True)

        if self.n_outputs != len(self.classes):
            raise ValueError(f'''the number of outputs must equal the number of classes in the training data.''')


        def error_convert(negLL):
            #print(negLL)
            return np.exp(-negLL)
        
        
        T_encode = self.encoding_(T)
        
        #optimizer = opt.nnOptimizers(self.all_weights).sgd
        error_trace = self.sgd(self.neg_log_likelihood, self.backward, [Xst, T_encode],\
                                n_epochs, learning_rate, verbose=verbose)   
                
        return self
    
    def softmax(self, Y):
        expY = np.exp(Y - max(0, np.max(Y)))
        denom = np.sum(expY, axis=1).reshape((-1, 1)) 
        Y = expY / (denom + sys.float_info.epsilon)        
        return Y


    def neg_log_likelihood(self, Xst, T):
        Ys = self.forward(Xst)
        loss = -np.mean(T * (np.log(Ys[-1]+ sys.float_info.epsilon)))
        return loss
        
    
    def use(self, X, return_hidden_layer_outputs=False):
        Xst = self.standardize_X(X)
        Ys = self.forward(Xst)
        Y = Ys[-1]
        classes = np.argmax(Y , axis=1)
        Zs = Ys[:-1]
        return (classes, Y, Zs) if return_hidden_layer_outputs else (classes, Y)
    
    def encoding_(self, T):
        if T.ndim == 1:
            T = T.reshape((-1, 1))
        return (T == np.unique(T)).astype(int)

    def get_error_trace(self):
        return self.error_trace
    
    def accuracy_(self,pT,T):
        acc = sum(pT == np.squeeze(T))/len(T)
        return acc*100
